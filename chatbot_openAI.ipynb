{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qGXuUTGCCv0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c02c2875-bb16-4873-8299-de7d51da87ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.0.36-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
            "Collecting langchain-core<0.2.0,>=0.1.48 (from langchain_community)\n",
            "  Downloading langchain_core-0.1.48-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain_community)\n",
            "  Downloading langsmith-0.1.52-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.2.0,>=0.1.48->langchain_community)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.48->langchain_community)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.48->langchain_community) (2.7.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain_community)\n",
            "  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m917.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.48->langchain_community)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.48->langchain_community) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.48->langchain_community) (2.18.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain_community\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.5 jsonpatch-1.33 jsonpointer-2.4 langchain-core-0.1.48 langchain_community-0.0.36 langsmith-0.1.52 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.10.1 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "Y72_zSYoC2uE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ec36b1-33c6-4238-9293-547a9d76c932"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.17-py3-none-any.whl (867 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.36 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.36)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.48 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.48)\n",
            "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.52)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.48->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Installing collected packages: langchain-text-splitters, langchain\n",
            "Successfully installed langchain-0.1.17 langchain-text-splitters-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "oDp9m4GJC8HS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e406f675-ecfd-48f8-830f-83758ed767a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.0-py3-none-any.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.7.1)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.110.3-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.11.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.2)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.62.2)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.4)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<=7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (1.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=ef290ac588750f58c9673986a5082e67a854d60ba6d5d01dae4fc6b8bb20a2c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, python-dotenv, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, importlib-metadata, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.5.0 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.110.3 h11-0.14.0 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-7.0.0 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.17.3 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 opentelemetry-util-http-0.45b0 overrides-7.7.0 posthog-3.5.0 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.37.2 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtU6edbDDHbi",
        "outputId": "7042fed8-4c95-4b5f-8e04-d0bc11c705f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/171.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/171.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQwe-iTaH0Kk",
        "outputId": "c781d2c2-05dd-4fde-8881-73eef733af2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.4-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.46 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.1.48)\n",
            "Collecting openai<2.0.0,>=1.10.0 (from langchain_openai)\n",
            "  Downloading openai-1.24.1-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.3/312.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<1,>=0.5.2 (from langchain_openai)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (0.1.52)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (2.7.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.46->langchain_openai) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.10.0->langchain_openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.46->langchain_openai) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.46->langchain_openai) (3.10.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain_openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.46->langchain_openai) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain_openai) (2.0.7)\n",
            "Installing collected packages: httpcore, tiktoken, httpx, openai, langchain_openai\n",
            "Successfully installed httpcore-1.0.5 httpx-0.27.0 langchain_openai-0.1.4 openai-1.24.1 tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up packages"
      ],
      "metadata": {
        "id": "EL6j2S5kDqNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Langchain csv loader\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "\n",
        "#Langchain text splitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "#Langchain vector database\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "#import Langchain openAI\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "#define tools\n",
        "from langchain.tools import BaseTool, StructuredTool, tool\n",
        "from langchain.agents import create_tool_calling_agent\n",
        "\n",
        "\n",
        "# use openai to generate\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import os"
      ],
      "metadata": {
        "id": "4F2ooi2WDPdt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import NTLK tool to prepare for the development of sentiment analysis tool\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSnzaZLq5DLB",
        "outputId": "2c7497ab-d0d6-4728-9841-4533e770849f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to google drive to fetch the csv file containing the mental health dialogue data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSjQy1IbDpQp",
        "outputId": "f1e887e9-251a-4294-ca78-160298b4eb2b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use CSV loader to load the csv file\n",
        "loader = CSVLoader(file_path='/content/drive/MyDrive/mental_health_train.csv')\n",
        "\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "dIW1scm3D9hS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text splitter and create chunks"
      ],
      "metadata": {
        "id": "djR1JQHKEB6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split text into chunks\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=25,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ],
      "metadata": {
        "id": "9qO4KGGNEAEm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnIEfEcIEIeB",
        "outputId": "1458ecef-7e73-45e0-8146-bf6d3d8a5667"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 309, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 399, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 303, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 317, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 391, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 312, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 367, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 395, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 303, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 331, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 303, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 303, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 352, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 384, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 358, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 497, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 338, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 411, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 430, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 312, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 332, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 533, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 519, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 357, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 380, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 940, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 396, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 326, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 449, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 373, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 393, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 396, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 554, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 320, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 306, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 325, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 339, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 324, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 326, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 416, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 343, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 433, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 364, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 495, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 347, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 388, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 420, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 358, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 450, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 379, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 409, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 374, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 394, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 357, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 328, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 317, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 310, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 347, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 469, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 331, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 302, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 306, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 350, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 324, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 335, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 321, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 335, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 359, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 310, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 307, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 301, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 481, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 659, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 355, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 410, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 498, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 423, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 454, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 357, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 421, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 379, which is longer than the specified 300\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 342, which is longer than the specified 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the retriever, embedding, LLM model, vector database"
      ],
      "metadata": {
        "id": "XZeftBZ6EOGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use openai embedding model to transform the chuncks of the texts into embedding vecotrs\n",
        "embeddings =  OpenAIEmbeddings(openai_api_key=\"api_key\")\n",
        "index = Chroma.from_documents(chunks, embeddings) #it takes nearly three minutes"
      ],
      "metadata": {
        "id": "clBwUAYSEJ0c"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = index.as_retriever()"
      ],
      "metadata": {
        "id": "L4sdNiV8EQ2C"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Langchain embeddings and LLM model, here is GPT4\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "igFivgi0HwD1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(model_name=\"gpt-4\", openai_api_key=\"api_key\")"
      ],
      "metadata": {
        "id": "Nx62flp0Ho_S"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlOSSzBVIFSU",
        "outputId": "ff27a9c8-4f9e-4e81-d7e9-670ad0376259"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e445c336350>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e447cb82080>, model_name='gpt-4', openai_api_key=SecretStr('**********'), openai_proxy='')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define sentiment analysis tool"
      ],
      "metadata": {
        "id": "IEh5OTKng7eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool(\"judge_negative\", return_direct=True)\n",
        "def judgenegative(sentence):\n",
        "  '''\n",
        "  if the compund score is less than -0.5, then it is a very negative sentence, we raise an alert message to let the patient find professional treatment\n",
        "  '''\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "  compound_score = sia.polarity_scores(sentence)[\"compound\"]\n",
        "  if compound_score<-0.9:\n",
        "    return \"Please do not hesitate to find professional mental treatment!\""
      ],
      "metadata": {
        "id": "T6D-HYfojbLE"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(judgenegative.name)\n",
        "print(judgenegative.description)\n",
        "print(judgenegative.args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjX103wPmXf2",
        "outputId": "4661b6c2-098e-4e9f-aa65-0965ee4dff69"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "judge_negative\n",
            "judge_negative(sentence) - if the compund score is less than -0.5, then it is a very negative sentence, we raise an alert message to let the patient find professional treatment\n",
            "{'sentence': {'title': 'Sentence'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools=[judgenegative] #define the list of tools, currently we only have one tool in this list"
      ],
      "metadata": {
        "id": "rWvip8b3qJUe"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=model.bind_tools(tools) #combine the GPT4 model with this tool"
      ],
      "metadata": {
        "id": "YXOPOB-3ILnz"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation Buffer memory"
      ],
      "metadata": {
        "id": "-uX6gz48HobD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "#define a conversation buffer memory, which the GPT 4 model can generate new text based on past conversation, to make the flow more consistent\n",
        "memory = ConversationBufferWindowMemory(k=2,memory_key=\"history\",\n",
        "    input_key=\"question\")\n",
        "conversation_buf = ConversationChain(\n",
        "    llm=model,\n",
        "    memory=memory)"
      ],
      "metadata": {
        "id": "Oz4rcCXSHQe0"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# build question-answer chain"
      ],
      "metadata": {
        "id": "LyI4YMmawAef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "\n",
        "template = \"\"\"\n",
        "Given the context documents: {context} and the chat history{history}, come up with answers for the question {question}.\n",
        "Only return your compiled and sorted answer start by '<ASSISTANT>: '\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"context\", \"question\"],\n",
        "    template=template,\n",
        ")"
      ],
      "metadata": {
        "id": "aM49U2F3Hn3w"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"history\",\n",
        "    input_key=\"question\"\n",
        ")\n",
        "\n",
        "retrieval_chain = RetrievalQA.from_chain_type(model,\n",
        "                                              chain_type='stuff',\n",
        "                                              retriever=retriever,\n",
        "                                              chain_type_kwargs={\n",
        "                                                  \"prompt\": prompt,\n",
        "                                                  \"memory\": memory\n",
        "                                              })"
      ],
      "metadata": {
        "id": "oNSzaa3rIVqK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a chatbot and save chat history"
      ],
      "metadata": {
        "id": "W4fQHlp_waYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_complete_sentences(text):\n",
        "    '''\n",
        "    input: output from hugging face chatbot\n",
        "    function to return completed sentences.\n",
        "    '''\n",
        "    # Regular expression pattern to match complete sentences\n",
        "    pattern = r'([A-Z][^.]*[.])'\n",
        "    # Find all complete sentences using the regex pattern\n",
        "    complete_sentences = re.findall(pattern, text)\n",
        "    # Join the sentences back into a single string\n",
        "    return ' '.join(complete_sentences)\n"
      ],
      "metadata": {
        "id": "zdJALEy73hhT"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global chathistory\n",
        "chathistory=[] #store the past chat history\n",
        "\n",
        "def chatbot():\n",
        "  '''\n",
        "  This function is aimed to build an inerface to let human and chatbot communicate with each other.\n",
        "  return : the chat history in the string format\n",
        "  '''\n",
        "\n",
        "  print(\"Welcome to mental health chatbot, hope you have a great treatment!\"+\"\\n\")\n",
        "\n",
        "\n",
        "  question=input(\"Please type your question here! Type 'S' or 'Stop' to end the conversation! \"+\"\\n\")\n",
        "  query=question\n",
        "\n",
        "  while question!=\"S\" and question!=\"Stop\":\n",
        "\n",
        "    chathistory.append(\"Question: \"+question)\n",
        "\n",
        "    ai_msg=extract_complete_sentences (  retrieval_chain.run(query).split(\"<ASSISTANT>:\")[-1] )\n",
        "\n",
        "    if len(ai_msg)==0:\n",
        "      ai_msg=\"Please find professional mental assistance!\"\n",
        "\n",
        "    print(\"This is Chatbot's answer: \"+ai_msg)\n",
        "\n",
        "\n",
        "    chathistory.append(\"Answer: \"+ai_msg)\n",
        "\n",
        "    question=input(\"Do you have further question? \"+\"\\n\")\n",
        "    query=question\n",
        "\n",
        "  print('Goodbye, hope you enjoy the conversation!')\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "G2eWQDhhxRzx"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use GPT-4 model to make a summary prescription"
      ],
      "metadata": {
        "id": "ZCOxAlJdaeUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prescription( chathistory,api_key):\n",
        "  \"\"\"\n",
        "  Based on the dialogue between the patient and the chatbot, summarize a prescription containing the symptom of the patient and how to solve the problem\n",
        "  \"\"\"\n",
        "  prompt = \"Given the questions from human patient (marked by Question: ) and answers from the chatbot (marked by Answer: ), return a prescription containing the symptom of the patient, cause of the symptom, suggestions to solve the mental health issue in the form of bullet point :\\n\\n\" + chathistory\n",
        "\n",
        "  client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=api_key,\n",
        "    )\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt,\n",
        "          }\n",
        "      ],\n",
        "      model=\"gpt-4\",\n",
        "  )\n",
        "\n",
        "  summary = response.choices[0].message.content\n",
        "  return summary\n",
        "\n"
      ],
      "metadata": {
        "id": "JQFLPBce4P32"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Integrate whole pipeline"
      ],
      "metadata": {
        "id": "_Qb7nEqLegQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  chatbot()\n",
        "\n",
        "  chat_history=\" \".join(chathistory)\n",
        "\n",
        "  prescription_result=prescription(chat_history,\"api_key\")\n",
        "\n",
        "  print(\"The prescription is: \"+\"\\n\"+prescription_result)\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "EvF2klKT4StG"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv8K9D0Ue3ws",
        "outputId": "8d55bfac-55b7-4319-80a5-1aef7db1b1b3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to mental health chatbot, hope you have a great treatment!\n",
            "\n",
            "Please type your question here! Type 'S' or 'Stop' to end the conversation! \n",
            "Hi, shall I ask some question about making new friends at a new environment?\n",
            "This is Chatbot's answer: Of course! I'd be happy to help you with questions about making new friends in a new environment. It's a common challenge when moving to a new place or starting a new job or school.\n",
            "Do you have further question? \n",
            "I am too shy to reach out to new people.\n",
            "This is Chatbot's answer: I understand how you're feeling, and it's completely normal to feel shy or nervous about reaching out to new people. Here are some strategies that might help:\n",
            "\n",
            "1. Reach out to others: Even if it feels difficult, try to connect with friends or family through phone calls, video chats, or social media platforms. You don't have to meet in person if you're not comfortable yet. Just a simple message or call can make a big difference. Join online communities: Look for virtual groups or forums where you can share your interests and experiences. This could be a good way to meet new people in a less intimidating environment. Check in with people you know: When you’ve been isolated, it’s easy to feel that people are doing fine without you. But remember, it’s usually not true. Try to overcome this feeling and reach out to them. Stay Positive and Patient: Building meaningful connections takes time and effort. It's completely okay to take small steps at a time. Be patient with yourself and others, and try to stay positive throughout the process. Remember that it's okay to feel shy and it's okay to take your time. You're not alone in this, and there are many resources and people who can help.\n",
            "Do you have further question? \n",
            "What if I do not have common words with surrounding people since I am in a foreign country?\n",
            "This is Chatbot's answer: That's a very common challenge when living in a foreign country, and there are many ways to navigate it:\n",
            "\n",
            "1. Language learning: If language is a barrier, consider taking language classes or using language learning apps. This can also be a great way to meet others who are also learning the language. Connecting over shared interests: Even if you don't share a common language, shared interests can be a great way to connect. Look for local groups or events that align with your hobbies or passions. Cultural exchange: Embrace the opportunity to learn about a new culture and share your own. This can often lead to interesting conversations and connections. Patience and understanding: Communication might be difficult at times, but many people appreciate the effort you're making to connect with them. Be patient with yourself and others. Use technology: There are many translation apps and tools that can help bridge the language gap. Remember, everyone has been a newcomer at some point, and most people will understand and be patient with you. Don't be too hard on yourself, and give yourself credit for the steps you're taking to build new connections.\n",
            "Do you have further question? \n",
            "I feel too lonely and depressed in a foreign country!\n",
            "This is Chatbot's answer: I'm really sorry that you're feeling this way, but I'm unable to provide the help that you need. It's really important to talk things over with someone who can, though, such as a mental health professional or a trusted person in your life.\n",
            "Do you have further question? \n",
            "Stop\n",
            "Goodbye, hope you enjoy the conversation!\n",
            "The prescription is: \n",
            "Prescription:\n",
            "\n",
            "Symptoms:\n",
            "- Patient feels shy and nervous about making new friends in a new environment.\n",
            "- Language and cultural barriers are causing difficulty in connecting with people.\n",
            "- Patient is feeling loneliness and depression after moving to a foreign country.\n",
            "\n",
            "Cause: \n",
            "- Moving to a new environment has led to isolation and difficulties interacting with others.\n",
            "- Lack of a common language is causing communication challenges.\n",
            "- The unfamiliarity and separation from known surroundings and people have resulted in loneliness and depression.\n",
            "\n",
            "Suggestions:\n",
            "- For shyness and difficulties in reaching out: \n",
            "    - Try initially connecting with friends or family through phone calls, video chats, or social media.\n",
            "    - Join online communities with shared interests or experiences.\n",
            "    - Regular check-in with known people to overcome the feeling of isolation.\n",
            "    - Remain positive and patient as building meaningful connections takes time.\n",
            "\n",
            "- For language and cultural barriers: \n",
            "    - Take language classes or use language learning apps.\n",
            "    - Look for local groups or events that align with your hobbies or passions.\n",
            "    - Embrace cultural exchange opportunities. \n",
            "    - Use translation apps and tools to help with communication.\n",
            "    - Be patient and understanding with yourself and others.\n",
            "\n",
            "- For feelings of loneliness and depression: \n",
            "    - Immediately seek help from a mental health professional or a trusted person in your life. \n",
            "    - Don't be too hard on oneself and give credit for the effort being made to adapt to the new environment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S7faWu9mnj7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mN9CcqTX4Swd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WMbL-KFdoTbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ils0FFfxoTeQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}